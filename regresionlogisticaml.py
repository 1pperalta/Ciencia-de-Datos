# -*- coding: utf-8 -*-
"""RegresionLogisticaML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h7aXW5dzrsyZAYnnI5tJpL9tK4Ti6UeW

PABLO DAVID PERALTA 000484263
"""

import seaborn as sns
import numpy as np
import pandas as pd


iris = sns.load_dataset('iris')

# Convertir la columna en una columna numérica, 0,1,2
iris['species'] = iris['species'].astype('category')
iris['species'] = iris['species'].cat.codes

# Dividir los datos en conjunto de entrenamiento (80%) y prueba (20%)
np.random.seed(0)
msk = np.random.rand(len(iris)) < 0.8
datos_entrenamiento = iris[msk]
datos_prueba = iris[~msk]

# Definir las características y etiquetas para entrenamiento y prueba
X_entrenamiento = datos_entrenamiento[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y_entrenamiento = datos_entrenamiento['species'].values
X_prueba = datos_prueba[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y_prueba = datos_prueba['species'].values

# Función sigmoidal
def sigmoidal(x):
    return 1 / (1 + np.exp(-x))

# Función de costo para la regresión logística
def calcular_costo(theta, X, y):
    m = len(y)
    h = sigmoidal(X.dot(theta))
    costo = (-1/m) * (y.T.dot(np.log(h)) + (1 - y).T.dot(np.log(1 - h)))
    return costo

# Gradiente descendente para la regresión logística
def descenso_gradiente(X, y, theta, alpha, num_iters):
    m = len(y)
    historial_costos = []

    for i in range(num_iters):
        h = sigmoidal(X.dot(theta))
        gradiente = X.T.dot(h - y) / m
        theta -= alpha * gradiente
        costo = calcular_costo(theta, X, y)
        historial_costos.append(costo)

    return theta, historial_costos

# Entrenamiento de los tres clasificadores OVR
clasificadores = []

#OVR

# Clasificador Setosa vs [VersiColor, Virginica]
y_entrenamiento_setosa = (y_entrenamiento == 0).astype(int)
theta_setosa = np.zeros(X_entrenamiento.shape[1])
theta_setosa, historial_costos_setosa = descenso_gradiente(X_entrenamiento, y_entrenamiento_setosa, theta_setosa, alpha=0.01, num_iters=1000)
clasificadores.append(theta_setosa)

# Clasificador VersiColor vs [Setosa, Virginica]
y_entrenamiento_versicolor = (y_entrenamiento == 1).astype(int)
theta_versicolor = np.zeros(X_entrenamiento.shape[1])
theta_versicolor, historial_costos_versicolor = descenso_gradiente(X_entrenamiento, y_entrenamiento_versicolor, theta_versicolor, alpha=0.01, num_iters=1000)
clasificadores.append(theta_versicolor)

# Clasificador Virginica vs [VersiColor, Setosa]
y_entrenamiento_virginica = (y_entrenamiento == 2).astype(int)
theta_virginica = np.zeros(X_entrenamiento.shape[1])
theta_virginica, historial_costos_virginica = descenso_gradiente(X_entrenamiento, y_entrenamiento_virginica, theta_virginica, alpha=0.01, num_iters=1000)
clasificadores.append(theta_virginica)

# Función para predecir la clase de una flor dado un conjunto de características
def predecir_clase(clasificadores, X):
    probabilidades = [sigmoidal(X.dot(theta)) for theta in clasificadores]
    return np.argmax(probabilidades)

# Clasifica la flor con sus características
nueva_flor = np.array([4.6, 3.1, 1.6, 0.3])
clase_predicha = predecir_clase(clasificadores, nueva_flor)

# Buscar la clase predicha para saber a que tipo de flor pertenece
especies_mapping = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
especie_predicha = especies_mapping[clase_predicha]
print(f'La clasificación de la flor es: {especie_predicha}')