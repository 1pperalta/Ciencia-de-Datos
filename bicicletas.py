# -*- coding: utf-8 -*-
"""Bicicletas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16WrFf-PX3hVv0srV9na30TKSZh8kqlDP

Trabajo de técnicas avanzadas de análisis estadistico, análisis de teoria de la información para Siastema de prestamo de Bicicletas. Nicolás Jaramillo - Pablo Peralta
"""

!pip install scikit-learn
from sklearn.preprocessing import MinMaxScaler

import pandas as pd
data=pd.read_csv('https://robertohincapie.com/data/bike-sharing-demand.csv', sep=',')
data.head()
data=data.drop('datetime', axis=1)
datadum = pd.get_dummies(data, columns=['weather','season'], drop_first=True)
datosparafact = datadum.copy()
datosparafact = datosparafact.drop('count', axis=1)
datosparafact = datosparafact.drop('registered', axis=1)
division = datosparafact['casual']/data['count']
datosparafact['Porcentaje de Casual'] = division
datosparafact = datosparafact.drop('casual', axis=1)
scaler = MinMaxScaler()
columnas_a_normalizar = ['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']
datadum[columnas_a_normalizar] = scaler.fit_transform(datadum[columnas_a_normalizar])
datadum.head()

datosparafact.head()

datadum.var()

datadum.cov()

import numpy as np

np.linalg.eig(datadum.cov())

print('Varianza: ',datadum.cov().values.trace())
des=data['count'].values
datadum=datadum-datadum.mean()

from sklearn import decomposition
import seaborn as sns
import matplotlib.pyplot as plt

pca = decomposition.PCA()
pca.fit(datadum.values)
pca.components_[1]
#for proy, col in zip(pca.components_[21], data.columns):
#  print(proy, col)

Y=pca.transform(datadum.values)
#print(Y[0])

cols=datadum.columns
Y=pd.DataFrame(Y, columns=['PC_'+str(i) for i in range(1,len(cols)+1)])

plt.plot(range(1,len(cols)+1), np.cumsum(pca.explained_variance_ratio_),'x-b')
plt.xlabel('Número de componentes')
plt.ylabel('Porcentaje de la varianza')
plt.grid()
Y.head()

for k,v in zip(datadum.columns, pca.components_[1]):
  print(k,v)

import seaborn as sns
Y.head()
sns.scatterplot(data= Y, x= 'PC_1', y='PC_2', hue= data['count'])

"""ANALISIS DE FACTORES

"""

!pip install factor-analyzer

#Desde PCA ya conocemos que podemos trabajar con 3 o 4 componentes.
#Ya tenemos importado decomposition
#from sklearn import decomposition
#datosparafact = datosparafact.drop('count', axis=1)
from factor_analyzer import FactorAnalyzer
N=3
fa=FactorAnalyzer(n_factors=N, rotation='varimax')
fa.fit(datosparafact.values)
X2=fa.transform(datosparafact.values)

tmp=fa.loadings_
names=list(datosparafact.columns)
out=[]
cols=[]
fac=np.argmax(np.abs(tmp), axis=1)
for i in range(N):
  #Análisis del factor i-esimo
  print('Factor: ',i+1)
  tmp2=tmp[fac==i,:]
  names2=[names[int(a)] for a in list(np.where(fac==i)[0])]
  #print(tmp2, names2)
  vmax=np.max(np.abs(tmp2), axis=1)
  ind=np.argsort(-vmax)
  #print(ind)
  for j in range(len(ind)):
    cols.append(names2[ind[j]])
    out.append(tmp2[ind[j]])
    print('- '+names2[ind[j]]+'->'+str(tmp2[ind[j]][np.argmax(np.abs(tmp2[ind[j]]))]))

#for na, va in zip(cols, out):
#  print(na, va)
import seaborn as sns
plt.figure(figsize=(12,12))
sns.heatmap(out, annot=True)
borrar=plt.yticks(ticks=np.arange(len(cols))+0.5, labels=cols, rotation=0)

"""FACTORES

1.   Temperaturas
2.   Humedo y Viento
3. Verano y Otoño

ENTROPIA
"""

from sklearn.preprocessing import KBinsDiscretizer
def entropia(data, col):
  prob=data[col].value_counts()/len(data)
  return np.sum(prob*np.log2(1/prob))

def gini(data, col):
  prob=data[col].value_counts()/len(data)
  return 1-np.sum(prob**2)

def varianza(data, col):
  return data[col].var()

def InformacionCondicional(data, col, col2, funcion):
  Htot=0
  if(len(data[col2].unique())>20): #Hay demasiados valores discretos
    res=data[col2].values.reshape(-1, 1)
    kbd=KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')
    data['tmp']=kbd.fit_transform(res)
  else:
    data['tmp']=data[col2].copy()

  for ci in data['tmp'].unique():
    df2=data[data['tmp']==ci]
    Hcond=funcion(df2, col)
    Htot=Htot+Hcond*len(df2)/len(data)
  return Htot

def GananciaInformacion(df, col, col2, funcion):
  return funcion(df, col), funcion(df, col)-InformacionCondicional(df, col, col2, funcion)

dfgod = ['season',	'holiday',	'workingday',	'weather',	'temp',	'atemp','humidity',	'windspeed',	'casual',	'registered']
for i in dfgod:
    ganancia = GananciaInformacion(data, 'count', i, entropia)
    print(f"La ganancia de información de {i} con respecto a count es {ganancia[1]}")

