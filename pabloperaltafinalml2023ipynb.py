# -*- coding: utf-8 -*-
"""PabloPeraltaFINALML2023ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWgOpK6EIast8f5d3r8SdOHCxu7Kp52n

Trabajo Final de ML, Pablo David Peralta 000484263
"""

#Explicacion de la base de datos elegida

#-Escogi una base de datos de accidentalidad en envigado, en la cual utilizare como variable objetivo o dependiente la gravedad del accidente, donde existen 3 tipos (HERIDOS,MUERTOS,SOLO DANOS)
#Con respecto a las columnas independientes elegire el dia de la semana, causa, barrio, tipo de servicio, tipo de vehiculo y area urbana. Las demas las cuales son coordenadas, fecha, hora, y de ese estilo, seran eliminadas.

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Lee tu conjunto de datos
data= pd.read_csv('accidentesenvigado.csv')
data.head()

data.head()

#Eliminar columnas que no se utilizaran
columnas_a_eliminar = ['RADICADO', 'FECHA', 'HORA', 'SEXO','TIPO DE VICTIMA', 'ESTADO DE BEODEZ', 'RESULTADO DE BEODEZ','DIRECCIÓN', 'Coordenadas', 'CLASE DE VEHICULO','TIPO DE SERVICIO', 'CAUSA']
df= data.drop(columnas_a_eliminar, axis=1)

df.head()

columnas_a_dummies = ['DÍA DE LA SEMANA', 'CLASE DE ACCIDENTE', 'BARRIO', 'AREA',]
df = pd.get_dummies(df, columns=columnas_a_dummies, drop_first=True)

df.head()

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
df["GRAVEDAD"] = labelencoder.fit_transform(df["GRAVEDAD"])
df["GRAVEDAD"]

#Al tener solo variables categoricas, no es necesario hacer normalizacion.
df['GRAVEDAD'].nunique()
df['GRAVEDAD'].value_counts()

#Balanceo de DATOS con Smote
X = df.drop('GRAVEDAD', axis=1)
y = df['GRAVEDAD']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Aplicar SMOTE para generar datos sintéticos
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

df_sinteticos = pd.DataFrame(X_train_resampled, columns=X.columns)
df_sinteticos['GRAVEDAD'] = y_train_resampled

datos_combinados = pd.concat([df, df_sinteticos], ignore_index=True)

print("Número total de observaciones:", len(datos_combinados))

datos_combinados.head()

df1= datos_combinados.copy()

"""Despues de balancear, cogere 5000 datos de cada uno, para mayor"""

df1['GRAVEDAD'].value_counts()

num_samples = 5000
df1 = datos_combinados.groupby('GRAVEDAD').head(num_samples)
df1.reset_index(drop=True, inplace=True)
print("Número total de observaciones en el DataFrame seleccionado:", len(df1))

df1['GRAVEDAD'].value_counts()

X = df1.drop('GRAVEDAD', axis=1)
y = df1['GRAVEDAD']

# Dividir los datos en entrenamiento y prueba
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

"""RED Neuronal"""

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Definir el modelo MLP
modelo = MLPClassifier()

Hiperparametros = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.0005, 0.001],
    'learning_rate': ['constant', 'adaptive', 'invscaling'],
    'activation': ['identity', 'logistic', 'tanh', 'relu']
}
# Crear el objeto GridSearchCV
grid_search = GridSearchCV(modelo, Hiperparametros, cv=3, scoring='accuracy')
grid_search.fit(X_train1,y_train1)

#Mejores HiperParametros
mejores_Hiperparametros = grid_search.best_params_
print("Mejor Ajuste de Hipeparametro:", mejores_Hiperparametros)

y_pred = grid_search.predict(X_test1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Supongamos que ya has definido y_pred y y_test

# Calcular y mostrar métricas de clasificación
print("Exactitud (Accuracy):", accuracy_score(y_test1, y_pred))
print("Precisión (Precision):", precision_score(y_test1, y_pred, average='weighted'))
print("Recall:", recall_score(y_test1, y_pred, average='weighted'))
print("F1 Score:", f1_score(y_test1, y_pred, average='weighted'))

# Mostrar el informe de clasificación
print("\nInforme de clasificación:\n", classification_report(y_test1, y_pred))

# Calcular y mostrar la matriz de confusión
matriz_confusion = confusion_matrix(y_test1, y_pred)
print("\nMatriz de confusión:\n", matriz_confusion)

#11
Mejor_clasificador = grid_search.best_estimator_
print("Mejor configuracion de la red neuronal:", Mejor_clasificador)

#PESOS
for i, pesos_capa in enumerate(Mejor_clasificador.coefs_):
    print(f"Pesos de la Capa {i + 1}:")
    print(pesos_capa)

X_prueba = X_test1

# Realizar predicciones en los datos de prueba
predicciones = Mejor_clasificador.predict(X_prueba)

# Imprimir las predicciones
print("Predicciones:")
print(predicciones)

"""La mejor metrica de clasificacion seria la sensibilidad la cual es 68%, lo que significa una buena cifra si quisieramos hablar de falsos negativos y evitando casos como por ejemplo que el modelo prediga que fueron solo danos, sabiendo que en realidad esta muerto o quedo herido.

PREDICCION CON SOFTMAX
"""

import numpy as np

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def forward_propagation(X, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3):
    # Capa 1
    salida_capa1 = np.dot(X, pesos_capa1) + bias_capa1
    activacion_capa1 = softmax(salida_capa1)

    # Capa 2
    salida_capa2 = np.dot(activacion_capa1, pesos_capa2) + bias_capa2
    activacion_capa2 = softmax(salida_capa2)

    # Capa de salida (Softmax para tres clases)
    salida_capa_salida = np.dot(activacion_capa2, pesos_capa3) + bias_capa3
    activacion_capa_salida = softmax(salida_capa_salida)

    clases_predichas = np.argmax(activacion_capa_salida, axis=1)

    return clases_predichas

# pesos y datos de entrada para tres capas
pesos_capa1 = np.random.rand(62, 2)
bias_capa1 = np.random.rand(2)

pesos_capa2 = np.random.rand(2, 2)
bias_capa2 = np.random.rand(2)

pesos_capa3 = np.random.rand(2, 3)
bias_capa3 = np.random.rand(3)

# Datos de entrada
X_ejemplo = X_test1

#  forward propagation
prediccion = forward_propagation(X_ejemplo, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3)

print("Predicción:", prediccion)

"""PREDICCION SIGMOIDE"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def forward_propagation(X, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3):
    # Capa 1
    salida_capa1 = np.dot(X, pesos_capa1) + bias_capa1
    activacion_capa1 = sigmoid(salida_capa1)

    # Capa 2
    salida_capa2 = np.dot(activacion_capa1, pesos_capa2) + bias_capa2
    activacion_capa2 = sigmoid(salida_capa2)

    # Capa de salida (Softmax para tres clases)
    salida_capa_salida = np.dot(activacion_capa2, pesos_capa3) + bias_capa3
    activacion_capa_salida = sigmoid(salida_capa_salida)

    clases_predichas = np.argmax(activacion_capa_salida, axis=1)

    return clases_predichas

# Ejemplo de pesos y datos de entrada para tres capas
pesos_capa1 = np.random.rand(62, 2)
bias_capa1 = np.random.rand(2)

pesos_capa2 = np.random.rand(2, 2)
bias_capa2 = np.random.rand(2)

pesos_capa3 = np.random.rand(2, 3)
bias_capa3 = np.random.rand(3)

# Datos de entrada
X_ejemplo = X_test1

# Realizar forward propagation
prediccion = forward_propagation(X_ejemplo, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3)

print("Predicción:", prediccion)

"""PREDICCION relU"""

def relu(x):
    return np.maximum(0, x)


def forward_propagation(X, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3):
    # Capa 1
    salida_capa1 = np.dot(X, pesos_capa1) + bias_capa1
    activacion_capa1 = relu(salida_capa1)

    # Capa 2
    salida_capa2 = np.dot(activacion_capa1, pesos_capa2) + bias_capa2
    activacion_capa2 = relu(salida_capa2)

    # Capa de salida (Softmax para tres clases)
    salida_capa_salida = np.dot(activacion_capa2, pesos_capa3) + bias_capa3
    activacion_capa_salida = relu(salida_capa_salida)

    clases_predichas = np.argmax(activacion_capa_salida, axis=1)

    return clases_predichas

# Ejemplo de pesos y datos de entrada para tres capas
pesos_capa1 = np.random.rand(62, 2)
bias_capa1 = np.random.rand(2)

pesos_capa2 = np.random.rand(2, 2)
bias_capa2 = np.random.rand(2)

pesos_capa3 = np.random.rand(2, 3)
bias_capa3 = np.random.rand(3)

# Datos de entrada
X_ejemplo = X_test1

# Realizar forward propagation
prediccion = forward_propagation(X_ejemplo, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3)

print("Predicción:", prediccion)

def tanh(x):
    return np.tanh(x)

def forward_propagation(X, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3):
    # Capa 1
    salida_capa1 = np.dot(X, pesos_capa1) + bias_capa1
    activacion_capa1 = tanh(salida_capa1)

    # Capa 2
    salida_capa2 = np.dot(activacion_capa1, pesos_capa2) + bias_capa2
    activacion_capa2 = tanh(salida_capa2)

    # Capa de salida
    salida_capa_salida = np.dot(activacion_capa2, pesos_capa3) + bias_capa3
    activacion_capa_salida = tanh(salida_capa_salida)

    clases_predichas = np.argmax(activacion_capa_salida, axis=1)

    return clases_predichas

# Ejemplo de pesos y datos de entrada para tres capas
pesos_capa1 = np.random.rand(62, 2)
bias_capa1 = np.random.rand(2)

pesos_capa2 = np.random.rand(2, 2)
bias_capa2 = np.random.rand(2)

pesos_capa3 = np.random.rand(2, 3)
bias_capa3 = np.random.rand(3)

# Datos de entrada
X_ejemplo = X_test1

# Realizar forward propagation
prediccion = forward_propagation(X_ejemplo, pesos_capa1, bias_capa1, pesos_capa2, bias_capa2, pesos_capa3, bias_capa3)

print("Predicción:", prediccion)